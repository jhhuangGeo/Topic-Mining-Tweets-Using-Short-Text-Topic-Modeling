{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "special-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "#title:Short text topic modeling\n",
    "#author: Jing-Huei Huang\n",
    "#date: May 26, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")#, category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.options.display.max_rows = 999\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import seaborn as sbn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import preprocessor as p\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pprintpp import pprint\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "sbn.set(style='ticks', font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tweets raw datasets - geotagged tweets - pre-COVID\n",
    "dfgeo1_pre = pd.read_csv('../Tweet_preprocessing/bronx/bronx_geo_pre1_N5035.csv', header=5)\n",
    "dfgeo2_pre = pd.read_csv('../Tweet_preprocessing/bronx/bronx_geo_pre2_N1273.csv', header=5)\n",
    "\n",
    "#combine datasets\n",
    "dfgeo_pre  = pd.concat([dfgeo1_pre, dfgeo2_pre], axis=0).reset_index()\n",
    "\n",
    "# Read tweets raw dataset - tweets containing park names\n",
    "dfkey_pre = pd.read_csv('../Tweet_preprocessing/bronx/bronx_kw_pre_N906.csv', header=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read tweets raw datasets - geotagged tweets - post-COVID\n",
    "dfgeo1_post = pd.read_csv('../Tweet_preprocessing/bronx/bronx_geo_post1_N2775.csv', header=5)\n",
    "dfgeo2_post = pd.read_csv('../Tweet_preprocessing/bronx/bronx_geo_post2_N611.csv', header=5)\n",
    "\n",
    "#combine datasets\n",
    "dfgeo_post  = pd.concat([dfgeo1_post, dfgeo2_post], axis=0).reset_index()\n",
    "\n",
    "# Read tweets raw dataset - tweets containing park names\n",
    "dfkey_post = pd.read_csv('../Tweet_preprocessing/bronx/bronx_kw_post_N944.csv', header=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "\n",
    "def colsel(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by='Date', ascending=True).reset_index()\n",
    "    cols = ['Url', 'Date', 'Gender', 'Account Type', 'Twitter Verified', 'Author', 'Twitter Author ID', 'Hashtags', 'Longitude', 'Latitude', 'City','Location Name', 'Sentiment', 'Twitter Retweet of', 'Full Text']\n",
    "    df = df[cols]\n",
    "    df.columns = ['Url', 'Date', 'Gender', 'Account Type', 'Twitter Verified', 'Author', 'userID', 'Hashtags', 'Longitude', 'Latitude', 'City','Location Name', 'Sentiment', 'Twitter Retweet of', 'fulltext']\n",
    "    df['id'] = df['Url'].apply(lambda x: x.split('/')[-1])\n",
    "    df['Twitter Retweet of'] = df['Twitter Retweet of'].fillna(0)\n",
    "    df['hashtag_seg'] = df[df['Hashtags'].notnull()]['Hashtags'].apply(lambda x: x.split(','))\n",
    "    return df\n",
    "\n",
    "dfkey_pre = colsel(dfkey_pre)\n",
    "dfgeo_pre = colsel(dfgeo_pre)\n",
    "\n",
    "dfkey_post = colsel(dfkey_post)\n",
    "dfgeo_post = colsel(dfgeo_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-dayton",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfkey_pre.shape)\n",
    "print(dfgeo_pre.shape)\n",
    "\n",
    "print(dfkey_post.shape)\n",
    "print(dfgeo_post.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dfkey_pre['userID'].unique()))\n",
    "print(len(dfgeo_pre['userID'].unique()))\n",
    "\n",
    "print(len(dfkey_post['userID'].unique()))\n",
    "print(len(dfgeo_post['userID'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-avatar",
   "metadata": {},
   "source": [
    "## Next step: identify tweets geotagged in UG areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dbf files to extract overlapping tweets\n",
    "from simpledbf import Dbf5\n",
    "\n",
    "def arc(dfgeo, f, pp):\n",
    "    dbf = Dbf5(f)\n",
    "    dfdb = dbf.to_dataframe()\n",
    "    dfgeo = dfgeo[dfgeo['id'].isin(dfdb['tweetID'])]\n",
    "    print(pp+' : identify tweets in UG areas.')\n",
    "    print(dfgeo.shape, 'tweets in df')\n",
    "    print(len(dfgeo['userID'].unique()), 'unique user ID in df')\n",
    "    return dfgeo\n",
    "\n",
    "dfgeo_pre = arc(dfgeo_pre, '../Tweet_preprocessing/bronx/XYbronx_geo_inpark_pre_N2766.dbf', 'preCOVID')\n",
    "dfgeo_post = arc(dfgeo_post, '../Tweet_preprocessing/bronx/XYbronx_geo_inpark_post_N1133.dbf', 'postCOVID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-algorithm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate tweets\n",
    "dfpre  = pd.concat([dfkey_pre, dfgeo_pre], axis=0).reset_index()\n",
    "dfpre = dfpre[~dfpre.duplicated('id')]\n",
    "\n",
    "dfpost  = pd.concat([dfkey_post, dfgeo_post], axis=0).reset_index()\n",
    "dfpost = dfpost[~dfpost.duplicated('id')]\n",
    "\n",
    "dfkey = pd.concat([dfkey_pre, dfkey_post], axis=0).reset_index()\n",
    "dfgeo = pd.concat([dfgeo_pre, dfgeo_post], axis=0).reset_index()\n",
    "\n",
    "print(dfpre.shape)\n",
    "print(dfpost.shape)\n",
    "\n",
    "dfpre['COVID'] = 'pre'\n",
    "dfpost['COVID'] = 'post'\n",
    "\n",
    "dfall = pd.concat([dfpre, dfpost], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables\n",
    "dfall['inKeywords'] = dfall['id'].isin(dfkey['id'])\n",
    "dfall['inGeo']      = dfall['id'].isin(dfgeo['id'])\n",
    "dfall['inLong']     = dfall['Longitude']!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tweets that meet criteria - from individual accounts, excluded tweets from verified accounts and retweets\n",
    "dfall = dfall[(dfall['Account Type']=='individual') & -(dfall['Twitter Verified']) & (dfall['Twitter Retweet of']==0)]\n",
    "dfall.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-athletics",
   "metadata": {},
   "source": [
    "## 1st Cleanning tweets: remove bot/ad/scam tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns for num_vocab, num_userID, num_RedunTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfall['original_fulltext'] = dfall['fulltext']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-duration",
   "metadata": {},
   "source": [
    "# remove junk tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetRemoveKeys = ['amazon ccbeauty','ccbeauty flash','focus foundation',\n",
    "                   'gel liner','flash palette','coffee wetnwildbeauty',\n",
    "                   'water outage','photo focus','hot water',\n",
    "                   'wetnwildbeauty photo', 'residential tenant',\n",
    "                   'bronx park east']\n",
    "                   #'@ bronx park east',\n",
    "                   #'bronx park east station',\n",
    "                   #'near bronx park east',\n",
    "                   #'bronx park east:',\n",
    "                   #'approaching bronx park east',\n",
    "                   #'at bronx park east'] \n",
    "\n",
    "for tk in tweetRemoveKeys:\n",
    "    matched=dfall['original_fulltext'].str.contains(tk, flags=re.IGNORECASE)\n",
    "    print('Current keywords to romove: %s' % tk)\n",
    "    print('Keywords matched: %d tweets' % sum(matched))\n",
    "    print('Matched tweet full text\\n')\n",
    "    print(dfall[matched]['original_fulltext'].values)\n",
    "    dfall = dfall[-matched].reset_index(drop=True)\n",
    "    print('Remained dfall size: %d tweets' % dfall.shape[0])\n",
    "    print('*****************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to get the number of redundant tweet per userID\n",
    "# Also exclude \n",
    "\n",
    "dfall['num_RedunTweet_perUser'] = dfall.groupby(['userID', 'fulltext'])['id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-generator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get tweets posted by users who have more than 2 redundant tweets \n",
    "dfall['MoreThan2RedunTweets_perUser']= dfall['num_RedunTweet_perUser']>2\n",
    "dfall['MoreThan2RedunTweets_perUser'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topUremove(df):\n",
    "    # Number of tweet per user in current (or remained) dataframe\n",
    "    df['numTweet_perUser'] = df.groupby(['userID'])['id'].transform('count')\n",
    "\n",
    "    # Top 5% most numTweet users\n",
    "    numUsers = len(df['userID'].unique())\n",
    "    rank5 = int(np.ceil(numUsers*0.05))\n",
    "    rank5numTweetCutoff = sorted(df.groupby('userID')['numTweet_perUser'].first().to_list(), reverse=True)[rank5]\n",
    "    print('Top 5%% User removed when numTweet_perUser is greater than %d' % rank5numTweetCutoff)\n",
    "    df['rank5remove'] = df['numTweet_perUser']>=rank5numTweetCutoff\n",
    "    print(df['rank5remove'].value_counts())\n",
    "    # Top 1% most numTweet users\n",
    "    numUsers = len(df['userID'].unique())\n",
    "    rank1 = int(np.ceil(numUsers*0.01))\n",
    "    rank1numTweetCutoff = sorted(df.groupby('userID')['numTweet_perUser'].first().to_list(), reverse=True)[rank1]\n",
    "    print('Top 1%% User removed when numTweet_perUser is greater than %d' % rank1numTweetCutoff)\n",
    "    df['rank1remove'] = df['numTweet_perUser']>=rank1numTweetCutoff\n",
    "    print(df['rank1remove'].value_counts())\n",
    "\n",
    "    return df\n",
    "\n",
    "dfe_tmp = topUremove(dfall[dfall['COVID']=='pre'])\n",
    "dfo_tmp = topUremove(dfall[dfall['COVID']=='post'])\n",
    "dfall = pd.concat([dfe_tmp, dfo_tmp], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "photographic-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column that show number of words in fulltext \n",
    "dfall['num_vocab'] = dfall['fulltext'].apply(lambda x: len(x.split(' ')) if x else 0)\n",
    "dfall['atleast_3_vocab']=dfall['num_vocab']>=3\n",
    "dfall['atleast_3_vocab'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select pre or post here\n",
    "dfselo = dfall[-(dfall['MoreThan2RedunTweets_perUser']) & -(dfall['rank1remove']) & (dfall['num_vocab']>=3) & (dfall['COVID']=='pre')]\n",
    "dfselo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfselo['Author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print wordcloud\n",
    "tweet_ALL = \" \".join(t for t in dfselo['fulltext'])\n",
    "fig, ax = plt.subplots(1,1, figsize=(30,10))\n",
    "#cloud_mask = np.array(Image.open(\"shapes\\cloud1.png\"))\n",
    "wc = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_ALL)\n",
    "ax.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-foundation",
   "metadata": {},
   "source": [
    "## 2nd Cleaning text content of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expensive-lender",
   "metadata": {},
   "source": [
    "# Create dataframe for a park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for a park\n",
    "dfsel_park = dfselo.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-companion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet cleaning - extract hashtags\n",
    "#dfsel_park['hashtag'] = dfsel_park['fulltext'].apply(lambda x: re.findall(r\"#(\\w+)\", x))\n",
    "# Part of ekphrasis package - segmenter and download twitter word stats\n",
    "seg_tw = Segmenter(corpus=\"twitter\")\n",
    "# Apply each hashtag to segmenter and return to a new column\n",
    "\n",
    "dfsel_park['Hashtags'] = dfsel_park['Hashtags'].fillna(' ')\n",
    "dfsel_park['hashtag_seg'] = dfsel_park['Hashtags'].apply(lambda x: x.replace(' ','').split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-letters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need validation if most of tweets don't have hashtags\n",
    "\n",
    "(dfsel_park.astype(str)['hashtag_seg'] == '[\\'\\']').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-oregon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "fdist = FreqDist([e for l in dfsel_park['hashtag_seg'] for e in l])\n",
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffreq = pd.DataFrame.from_records(fdist.most_common(), columns=['word', 'freq'])\n",
    "dffreq.to_csv('../output/pre/Bronx/Bronx_final_hashtagFreq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print wordcloud\n",
    "tweet_ALL = \" \".join([e.strip('\\#').lower() for l in dfsel_park['hashtag_seg'] for e in l if len(e)>0])\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,5))\n",
    "wc = WordCloud(max_font_size=80, max_words=100, background_color=\"white\").generate(tweet_ALL)\n",
    "ax.imshow(wc, interpolation='bilinear')\n",
    "fig.savefig('../output/pre/Bronx/20210619_bronxPark_Hashtag_wordcloud.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-prospect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet cleaning - tweet-preprocessor 0.6.0 \n",
    "# Executed cleaning using default, which went through all options\n",
    "#URL\tp.OPT.URL\n",
    "#Mention\tp.OPT.MENTION\n",
    "#Hashtag\tp.OPT.HASHTAG\n",
    "#Reserved Wordsp.OPT.RESERVED\n",
    "#Emoji\tp.OPT.EMOJI\n",
    "#Smiley\tp.OPT.SMILEY\n",
    "#Number\tp.OPT.NUMBER\n",
    "\n",
    "dfsel_park['cleantext'] = dfsel_park['fulltext'].apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to get the number of redundant cleantweet per userID\n",
    "# Also exclude \n",
    "\n",
    "dfsel_park['num_RedunCleanTweet_perUser'] = dfsel_park.groupby(['cleantext'])['id'].transform('count')\n",
    "dfsel_park = dfsel_park[dfsel_park['num_RedunCleanTweet_perUser']<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['time_EST'] = pd.to_datetime(dfsel_park['Date'],  utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['only_date'] = dfsel_park['time_EST'].apply(lambda x: x.date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['only_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-desire",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,4))\n",
    "dftmp = dfsel_park.reset_index(drop=True)\n",
    "#dftmp = dftmp.pivot_table(index=['year', 'month', 'date'], values='id', aggfunc='count')\n",
    "idx = pd.date_range(\"2019-03-01\", periods=300)\n",
    "dftmp['only_date']\n",
    "idnum = []\n",
    "for d in idx:\n",
    "    idnum.append((dftmp['only_date']==d.date()).sum())\n",
    "    #print(d.date(), (dftmp['only_date']==d.date()).sum())\n",
    "ax.bar(idx, idnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(15,4))\n",
    "keyword = 'bronx'\n",
    "dftmp = dfsel_park[dfsel_park['fulltext'].str.contains(keyword, flags=re.I)].reset_index(drop=True)\n",
    "#dftmp = dftmp.pivot_table(index=['year', 'month', 'date'], values='id', aggfunc='count')\n",
    "idx = pd.date_range(\"2019-03-01\", periods=153)\n",
    "idnum = []\n",
    "for d in idx:\n",
    "    idnum.append((dftmp['only_date']==d.date()).sum())\n",
    "    #print(d.date(), (dftmp['only_date']==d.date()).sum())\n",
    "ax.bar(idx, idnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional step]\n",
    "# Concatenate cleantext and hashtag_seg\n",
    "# Maybe later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-intention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually remove high frequency keywords that may skew the result\n",
    "# Change to park name before replace\n",
    "# dfsel_park['cleantext'] = dfsel_park['cleantext'].str.replace('prospect park', '', flags=re.IGNORECASE)\n",
    "#dfsel['fulltext'] = dfsel['fulltext'].str.replace('prospectpark', '', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-flexibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK module\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-simple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].str.replace('[0-9]+', '')\n",
    "# all text to lower case\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "korean-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) #keep English Character\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: remove_punc(x))\n",
    "#dfsel['hashtag_seg'] = dfsel['hashtag_seg'].apply(lambda x: remove_punc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-climate",
   "metadata": {},
   "source": [
    "## Make n-gram with nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dfsel_park['cleantext'].str.contains('childrens zoo',flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-liver",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsel_park[dfsel_park['cleantext'].str.contains('bronx zoo holiday',flags=re.IGNORECASE)]['fulltext'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-relation",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents([doc.split() for doc in dfsel_park['cleantext']])\n",
    "# Filter only those that occur at least 50 times\n",
    "finder.apply_freq_filter(5)\n",
    "bigram_scores = finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_documents([doc.split() for doc in dfsel_park['cleantext']])\n",
    "# Filter only those that occur at least 50 times\n",
    "finder.apply_freq_filter(5)\n",
    "trigram_scores = finder.score_ngrams(trigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "bigram_pmi.columns = ['bigram', 'pmi']\n",
    "bigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)\n",
    "\n",
    "trigram_pmi = pd.DataFrame(trigram_scores)\n",
    "trigram_pmi.columns = ['trigram', 'pmi']\n",
    "trigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-investigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = set(stopwords.words('english'))\n",
    "# Filter for bigrams with only noun-type structures\n",
    "def bigram_filter(bigram):\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in stop_word_list or bigram[1] in stop_word_list:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Filter for trigrams with only noun-type structures\n",
    "def trigram_filter(trigram):\n",
    "    tag = nltk.pos_tag(trigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n",
    "        return False\n",
    "    if trigram[0] in stop_word_list or trigram[-1] in stop_word_list or trigram[1] in stop_word_list:\n",
    "        return False\n",
    "    if 'n' in trigram or 't' in trigram:\n",
    "         return False\n",
    "    if 'PRON' in trigram:\n",
    "        return False\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-caribbean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n",
    "# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n",
    "filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n",
    "                                              bigram_filter(bigram['bigram'])\\\n",
    "                                              and bigram.pmi > 5, axis = 1)][:500]\n",
    "\n",
    "filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n",
    "                                                 trigram_filter(trigram['trigram'])\\\n",
    "                                                 and trigram.pmi > 5, axis = 1)][:500]\n",
    "\n",
    "\n",
    "bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n",
    "trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-encoding",
   "metadata": {},
   "source": [
    "## Curate ngram words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-linux",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_del = ['open air', 'cant wait', 'th stprospect', 'join us', \n",
    "#              'bandshell whats', 'th annual', 'ive seen', 'past saturday',\n",
    "#              'july th', 'last night', 'th street', 'feel like',\n",
    "#              'dont know', 'im gonna', 'last week', 'last weekend',\n",
    "#              'look like', 'im going']\n",
    "#\n",
    "#bigrams = [e for e in bigrams if e not in bigram_del]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated bigrams after word analyses\n",
    "\n",
    "bigrams = [\n",
    " 'nybg',\n",
    " 'bronx park',\n",
    " 'bronx river',\n",
    " 'bronx zoo',\n",
    " 'van cortlandt',\n",
    " 'orchid show', # redundant\n",
    " 'cherry blossoms',\n",
    " 'brady playground',\n",
    " 'pelham bay',\n",
    " 'dinosaur safari',\n",
    " 'living art',\n",
    " 'sea lion',\n",
    " 'last night',\n",
    " 'happy birthday',\n",
    " 'holiday light',\n",
    " 'holiday train',\n",
    " 'mothers day',\n",
    " 'train show',\n",
    " #'orchid show', # redundant\n",
    " 'family fun',\n",
    " 'new york',\n",
    " 'botanical garden',\n",
    " 'rock garden',\n",
    " 'rose garden',\n",
    " 'york city',\n",
    " 'east bronx'\n",
    "]\n",
    "\n",
    "# make these singular: q trains, water fountains, bike lanes, new years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-fiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-routine",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-surprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated trigrams after word analyses\n",
    "trigrams = [\n",
    " 'summer end cityscape',\n",
    " 'roberto burle mar',\n",
    " 'rockefeller rose garden',\n",
    " 'holiday train show',\n",
    " 'bronx park east',\n",
    " 'New York Botanical Garden',\n",
    " 'Botanical Garden',\n",
    " 'new york city',\n",
    " 'ny botanical garden'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-characteristic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-grams. updated on 0618.2021 \n",
    "def create_ngram(x):\n",
    "    ngram = []\n",
    "    pickedgrams = []\n",
    "    for gram in trigrams:\n",
    "        if gram in x:\n",
    "            pickedgrams.append(gram)\n",
    "            ngram.append('_'.join(gram.split()))\n",
    "    for gram in bigrams:\n",
    "        anchor=True\n",
    "        for p in pickedgrams:\n",
    "            if (gram in p):\n",
    "                anchor=False\n",
    "        if anchor and (gram in x):\n",
    "            ngram.append('_'.join(gram.split()))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loved-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate n-grams\n",
    "def replace_ngram(x):\n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '')\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsel_park['ngrams'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-brain",
   "metadata": {},
   "source": [
    "## Move bigram/trigram to another column\n",
    "## Then delete ngram words in tweets cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['ngrams'] = dfsel_park['cleantext'].map(lambda x: create_ngram(x))\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].map(lambda x: replace_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer using nltk\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: lemmatize_text(x))\n",
    "#dfsel['hashtag_seg'] = dfsel['hashtag_seg'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-poverty",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-English words\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "def remove_nonEnglish(token):\n",
    "    return([w for w in token if w.lower() in words or not w.isalpha()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: remove_nonEnglish(x))\n",
    "#dfsel['hashtag_seg'] = dfsel['hashtag_seg'].apply(lambda x: remove_nonEnglish(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSmallVocab(token):\n",
    "    return([w for w in token if ((len(w)>2) & (len(w)<16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: removeSmallVocab(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-harmony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('th')\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@', 'en', 'de', 'wa', 'dont', 'cant', 's', 'nt']\n",
    "stop_words = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: [t for t in x if t not in stop_words])\n",
    "#dfsel['hashtag_seg'] = dfsel['hashtag_seg'].apply(lambda x: [t for t in x if t not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-bailey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-affiliate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer using spacy\n",
    "dfsel_park['clean_text_trigrams'] = dfsel_park['cleantext'].copy()\n",
    "allowed_tags=['NOUN', 'VERB']\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "dfsel_park['clean_text_trigrams'] = dfsel_park['clean_text_trigrams'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x)) if token.pos_ in allowed_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "american-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove smallvocab\n",
    "def removeSmallVocab(token):\n",
    "    return([w for w in token if ((len(w)>2) & (len(w)<16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams'] = dfsel_park['clean_text_trigrams'].apply(lambda x: removeSmallVocab(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blind-concern",
   "metadata": {},
   "source": [
    "## Merge single words and bigram/trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-equipment",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams'] = dfsel_park['clean_text_trigrams'] + dfsel_park['ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column that show number of words in fulltext \n",
    "dfsel_park['num_vocab'] = dfsel_park['clean_text_trigrams'].apply(lambda x: len(x) if x else 0)\n",
    "# remove empty rows and words length less than 2\n",
    "dfsel_park = dfsel_park[(dfsel_park['num_vocab']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ALL = \" \".join(t for t in dfsel_park['clean_text_trigrams'].apply(lambda x: ' '.join(map(str, x))))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(30,30))\n",
    "# makes the circle using numpy\n",
    "x, y = np.ogrid[:300, :300]\n",
    "#mask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\n",
    "#mask = 255 * mask.astype(int)\n",
    "cloud_mask = np.array(Image.open('shapes/cloud1.png'))\n",
    "wc = WordCloud(max_font_size=200, max_words=100, background_color=\"white\", \n",
    "              mask=cloud_mask, contour_width=3).generate(tweet_ALL)\n",
    "\n",
    "ax.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist([e for l in dfsel_park['ngrams'] for e in l])\n",
    "fdist.most_common()\n",
    "#len(fdist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsel_park['hashtag_seg'] \n",
    "#fdist_hashtag = FreqDist([e for l in dfsel_park['hashtag'] for e in l])\n",
    "#fdist_hashtag.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-restriction",
   "metadata": {},
   "source": [
    "# Copy ready to use dataframe into a dataframe named dfsel_park_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-mixer",
   "metadata": {},
   "source": [
    "# Select regions of survey (NYC, by borough, by park)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut low frequency words\n",
    "\n",
    "\n",
    "freq_cut = 3 # np.ceil(dfsel_park.shape[0]*0.0001)\n",
    "cut_words = []\n",
    "for v, freq in fdist.most_common():\n",
    "    if freq<=freq_cut:\n",
    "        cut_words.append(v)\n",
    "#dfboro['clean_text_trigrams_cutlowfreq'] = dfboro['clean_text_trigrams'].apply(lambda x: [e for e in x if e not in cut_words])        \n",
    "dfsel_park['clean_text_trigrams_cutlowfreq'] = dfsel_park['clean_text_trigrams'].apply(lambda x: [e for e in x if e not in cut_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dynamic-clerk",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist([e for l in dfsel_park['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "#fdist.most_common()\n",
    "len(fdist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-regular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column that show number of words in fulltext \n",
    "dfsel_park['num_vocab'] = dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: len(x) if x else 0)\n",
    "# remove empty rows and words length less than 2\n",
    "dfsel_park = dfsel_park[(dfsel_park['num_vocab']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['num_vocab'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-ethnic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsel_park['clean_text_trigrams_cutlowfreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist([e for l in dfsel_park['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "plt.hist(fdist.values(), bins=30, range=(0,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ALL = \" \".join(t for t in dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: ' '.join(map(str, x))))\n",
    "fig, ax = plt.subplots(1,1, figsize=(30,10))\n",
    "wc = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_ALL)\n",
    "ax.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist([e for l in dfsel_park['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "fdist.most_common()\n",
    "#len(fdist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dffreq = pd.DataFrame.from_records(fdist.most_common(), columns=['word', 'freq'])\n",
    "dffreq.to_csv('../output/pre/Bronx/bronx_pre_wordfrequency.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-psychiatry",
   "metadata": {},
   "source": [
    "## Make decision on which subset of tweets for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-syndrome",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['inLong'] = dfsel_park['Longitude']!=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park[['inKeywords', 'inGeo', 'inLong']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save current dfsel_park to a ori\n",
    "dfsel_park_static = dfsel_park.copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turned-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park = dfsel_park_static[dfsel_park_static['inGeo']|dfsel_park_static['inKeywords']]\n",
    "dfsel_park = dfsel_park.reset_index(drop=True)\n",
    "dfsel_park.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    " #dfsel_park_static['Longitude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-veteran",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_ALL = \" \".join(t for t in dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: ' '.join(map(str, x))))\n",
    "fig, ax = plt.subplots(1,1, figsize=(30,10))\n",
    "wc = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_ALL)\n",
    "ax.imshow(wc, interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist([e for l in dfsel_park['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "len(fdist.most_common())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep oritinal clean text final\n",
    "dfsel_park['clean_text_trigrams_cutlowfreq_ori'] = dfsel_park['clean_text_trigrams_cutlowfreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-swimming",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_words = ['bronx_park','day','time','today','year','let','come','park',\n",
    "                'find','take','way','thing',\n",
    "                'new_york','new_york_city','york_city','yesterday','tomorrow','doe'] #add'new yor'bc it could result in not informative topics\n",
    "\n",
    "dfsel_park['clean_text_trigrams_cutlowfreq'] = dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: [e for e in x if e not in remove_words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-differential",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist([e for l in dfsel_park['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "\n",
    "#dffreq = pd.DataFrame.from_records(fdist.most_common(), columns=['word', 'freq'])\n",
    "#dffreq.to_csv('../../Tweet_TM_result/STTM_model/20210424_Prospect_final_wordFreq.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-israeli",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdist = FreqDist([e for l in dfsel_park['hashtag_seg'] for e in l])\n",
    "# fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executive-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column that show number of words in fulltext \n",
    "dfsel_park['num_vocab'] = dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: len(x) if x else 0)\n",
    "# remove empty rows\n",
    "dfsel_park = dfsel_park[(dfsel_park['num_vocab']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-logging",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modified-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base and Cleaning \n",
    "import json\n",
    "import requests\n",
    "import emoji\n",
    "import regex\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "#Visualizations\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import pyLDAvis.gensim\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py \n",
    "import chart_studio.tools as tls\n",
    "\n",
    "#Natural Language Processing (NLP)\n",
    "import spacy\n",
    "import gensim\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "warnings.filterwarnings(\"ignore\")#, category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-conditions",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(dfsel_park['clean_text_trigrams_cutlowfreq'])\n",
    "print(len(id2word))\n",
    "corpus = [id2word.doc2bow(d) for d in dfsel_park['clean_text_trigrams_cutlowfreq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "from corextopic import corextopic as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-malawi",
   "metadata": {},
   "source": [
    "# Topic modeling STTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-darkness",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.path.append('../../pyCode/gits/gsdmm/')\n",
    "# sys.path.append('../../pyCode/gits/gsdmm/gsdmm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arctic-gates",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../../chapter3/gits/gsdmm/')\n",
    "sys.path.append('../../chapter3/gits/gsdmm/gsdmm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-presence",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks = list(range(100,650,50))\n",
    "# kstable = []\n",
    "# for k in ks:\n",
    "#     mgp = MovieGroupProcess(K=k, alpha=0.1, beta=0.1, n_iters=20)\n",
    "#     l_id2word = set(list(id2word.values()))\n",
    "#     y = mgp.fit(dfsel_park['clean_text_trigrams_cutlowfreq'].to_list(), len(l_id2word))\n",
    "#     with open('../output/pre/Bronx/bronx_kw_%s.model' % k, 'wb') as f:\n",
    "#         pickle.dump(mgp, f)\n",
    "#         f.close()\n",
    "#     clus = (np.array(mgp.cluster_doc_count)>0).sum()\n",
    "#     kstable.append({'k': k,\n",
    "#                     'clusters': clus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-request",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfks = pd.DataFrame(kstable)\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.plot(dfks['k'], dfks['clusters'], '.-', markersize=10)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Number of cluster found by GDSMM')\n",
    "plt.savefig('../output/pre/Bronx/bronx_pre_STTM_modeltest_0101',bbox_inches =\"tight\",pad_inches = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-accreditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "mgp = MovieGroupProcess(K=300,alpha=0.1, beta=0.1, n_iters=100) #updated parameters\n",
    "l_id2word = set(list(id2word.values()))\n",
    "y = mgp.fit(dfsel_park['clean_text_trigrams_cutlowfreq'].to_list(), len(l_id2word))\n",
    "with open('../output/pre/Bronx/bronx_pre_0101_k300', 'wb') as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "nt = (doc_count>0).sum()\n",
    "print('Number of topic that has tweets:', nt)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-nt:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "# Show the top 5 words in term frequency for each cluster \n",
    "#top_words(mgp.cluster_word_distribution, top_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-battlefield",
   "metadata": {},
   "outputs": [],
   "source": [
    "(doc_count>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def topic_words(cwd, ti, wc=5):\n",
    "    twords = []\n",
    "    for t in ti:\n",
    "        nwd = {k: v for k, v in sorted(cwd[t].items(), key=lambda item: item[1], reverse=True)}\n",
    "        n_items = list(islice(nwd.items(), 20))\n",
    "        #print('Topic number %s' % t)\n",
    "        for k,v in n_items:\n",
    "            twords.append({'fulltext': k,\n",
    "                           'topic_prob': v})\n",
    "            #print(k, v)\n",
    "        #print('\\n')\n",
    "    return twords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_assign = []\n",
    "\n",
    "for t in dfsel_park['clean_text_trigrams_cutlowfreq'].tolist():\n",
    "    score = mgp.choose_best_label(t)\n",
    "    clu_assign.append({'topic_number': score[0],\n",
    "                       'topic_prob': score[1]})\n",
    "\n",
    "dfclu = pd.DataFrame(clu_assign)\n",
    "dfsel_ppp = pd.concat([dfsel_park.reset_index(drop=True), dfclu], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-illustration",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('../output/pre/Bronx/bronx_pre_sttmmodel_topic_0629.xlsx', engine='xlsxwriter') as writer:\n",
    "    for k, g in dfsel_ppp.groupby('topic_number'):\n",
    "        gtmp = g.sort_values(by='topic_prob', ascending=False).reset_index(drop=True)\n",
    "        twords = topic_words(mgp.cluster_word_distribution, [k,], 10)\n",
    "        dftw = pd.DataFrame(twords)\n",
    "        #print(dftw.head())\n",
    "        dftw = pd.concat([dftw, gtmp[['original_fulltext', 'topic_prob']]], axis=0)\n",
    "        print(dftw.shape)\n",
    "        dftw.to_excel(writer, sheet_name='Topic%s'%k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored-kazakhstan",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_ppp.to_csv('../output/pre/Bronx/tweets_STTMresult_bronx_pre_0629.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trying-program",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_ppp['topic_number'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print wordcloud\n",
    "\n",
    "tweet_ALL = \" \".join([e for l in dfsel_ppp[dfsel_ppp['topic_number']==43]['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "fig, ax = plt.subplots(1,1, figsize=(30,10))\n",
    "wc = WordCloud(width=2400, height=1200, max_words=400, background_color=\"white\").generate(tweet_ALL) #max_font_size=60\n",
    "ax.imshow(wc, interpolation='none')\n",
    "fig.savefig('test_wordcloud.png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-hungary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.pivot_table(data=dfsel_ppp, index='topic_number', columns=['inKeywords', 'inGeo', 'inLong'], values='id', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsel_ppp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_ppp['categorize'] = np.random.choice(range(0,5), dfsel_ppp.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_ppp['Date'] = dfsel_ppp['Date'].dt.tz_localize('utc').dt.tz_convert('US/Eastern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suffering-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_ppp['categorize'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
