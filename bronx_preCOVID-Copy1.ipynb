{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "#title:Short text topic modeling\n",
    "#author: Jing-Huei Huang\n",
    "#date: May 26, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-employment",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")#, category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cognitive-processor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.options.display.max_rows = 999\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import string\n",
    "import seaborn as sbn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import preprocessor as p\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pprintpp import pprint\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "sbn.set(style='ticks', font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-cycling",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get dataset for the analysis:\n",
    "## tweets geotagged in parks per-COVID, tweets geotagged in parks post- COVID\n",
    "## tweets that mentioned park names per-COVID, tweets that mentioned park names post-COVID \n",
    "# Read tweets raw datasets - geotagged tweets - pore-COVID\n",
    "\n",
    "dfgeo_pre = pd.read_csv('../Tweet_preprocessing/bronx/bronx_geo_pre.csv', header=5)\n",
    "# Read tweets raw datasets - geotagged tweets - post-COVID\n",
    "dfgeo_post = pd.read_csv('../Tweet_preprocessing/bronx/bronx_geo_post.csv', header=5)\n",
    "# Read tweets raw datasets - tweets that mentioned park names pre-COVID \n",
    "dfkey_pre = pd.read_csv('../Tweet_preprocessing/bronx/bronx_kw_pre.csv', header=5)\n",
    "# Read tweets raw datasets - tweets that mentioned park names post-COVID \n",
    "dfkey_post = pd.read_csv('../Tweet_preprocessing/bronx/bronx_kw_post.csv', header=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "\n",
    "def colsel(df):\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.sort_values(by='Date', ascending=True).reset_index()\n",
    "    cols = ['Url', 'Date', 'Gender', 'Account Type', 'Twitter Verified', 'Author', 'Twitter Author ID', 'Hashtags', 'Longitude', 'Latitude', 'City','Location Name', 'Sentiment', 'Twitter Retweet of', 'Full Text']\n",
    "    df = df[cols]\n",
    "    df.columns = ['Url', 'Date', 'Gender', 'Account Type', 'Twitter Verified', 'Author', 'userID', 'Hashtags', 'Longitude', 'Latitude', 'City','Location Name', 'Sentiment', 'Twitter Retweet of', 'fulltext']\n",
    "    df['id'] = df['Url'].apply(lambda x: x.split('/')[-1])\n",
    "    df['Twitter Retweet of'] = df['Twitter Retweet of'].fillna(0)\n",
    "    df['hashtag_seg'] = df[df['Hashtags'].notnull()]['Hashtags'].apply(lambda x: x.split(','))\n",
    "    return df\n",
    "\n",
    "dfkey_pre = colsel(dfkey_pre)\n",
    "dfgeo_pre = colsel(dfgeo_pre)\n",
    "\n",
    "dfkey_post = colsel(dfkey_post)\n",
    "dfgeo_post = colsel(dfgeo_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-improvement",
   "metadata": {},
   "source": [
    "## Next step: identify tweets geotagged in UG areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dbf files to extract overlapping tweets\n",
    "from simpledbf import Dbf5\n",
    "\n",
    "def arc(dfgeo, f, pp):\n",
    "    dbf = Dbf5(f)\n",
    "    dfdb = dbf.to_dataframe()\n",
    "    dfgeo = dfgeo[dfgeo['id'].isin(dfdb['tweetID'])]\n",
    "    print(pp+' : identify tweets in UG areas.')\n",
    "    print(dfgeo.shape, 'tweets in df')\n",
    "    print(len(dfgeo['userID'].unique()), 'unique user ID in df')\n",
    "    return dfgeo\n",
    "\n",
    "dfgeo_pre = arc(dfgeo_pre, '../Tweet_preprocessing/bronx/XYbronx_geo_inpark_pre_N2766.dbf', 'preCOVID')\n",
    "dfgeo_post = arc(dfgeo_post, '../Tweet_preprocessing/bronx/XYbronx_geo_inpark_post_N1133.dbf', 'postCOVID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-cartoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variables# combine datasets and remove duplicate tweets\n",
    "# combine pre-COVID datasets\n",
    "dfpre  = pd.concat([dfkey_pre, dfgeo_pre], axis=0).reset_index()\n",
    "dfpre = dfpre[~dfpre.duplicated('id')]\n",
    "# combine post-COVID datasets\n",
    "dfpost  = pd.concat([dfkey_post, dfgeo_post], axis=0).reset_index()\n",
    "dfpost = dfpost[~dfpost.duplicated('id')]\n",
    "\n",
    "# create a variable that indicate if the tweets posted pre- or post COVID\n",
    "dfpre['COVID'] = 'pre'\n",
    "dfpost['COVID'] = 'post'\n",
    "\n",
    "# combine tweets geotagged in parks into 1 datatable and tweets that mentioned park names into a datatable   \n",
    "dfkey = pd.concat([dfkey_pre, dfkey_post], axis=0).reset_index()\n",
    "dfgeo = pd.concat([dfgeo_pre, dfgeo_post], axis=0).reset_index()\n",
    "\n",
    "# combine all into one dataset\n",
    "dfall = pd.concat([dfpre, dfpost], axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select tweets that meet criteria - from individual accounts, excluded tweets from verified accounts and retweets\n",
    "dfall = dfall[(dfall['Account Type']=='individual') & -(dfall['Twitter Verified']) & (dfall['Twitter Retweet of']==0)]\n",
    "dfall.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-clothing",
   "metadata": {},
   "source": [
    "## 1st Cleanning tweets: remove bot/ad/scam tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-century",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove junk tweets based on keywords that appear to be irrelavant\n",
    "tweetRemoveKeys = ['amazon ccbeauty','ccbeauty flash','focus foundation',\n",
    "                   'gel liner','flash palette','coffee wetnwildbeauty',\n",
    "                   'water outage','photo focus','hot water',\n",
    "                   'wetnwildbeauty photo', 'residential tenant',\n",
    "                   'bronx park east']\n",
    "                   #'@ bronx park east',\n",
    "                   #'bronx park east station',\n",
    "                   #'near bronx park east',\n",
    "                   #'bronx park east:',\n",
    "                   #'approaching bronx park east',\n",
    "                   #'at bronx park east'] \n",
    "\n",
    "for tk in tweetRemoveKeys:\n",
    "    matched=dfall['original_fulltext'].str.contains(tk, flags=re.IGNORECASE)\n",
    "    print('Current keywords to romove: %s' % tk)\n",
    "    print('Keywords matched: %d tweets' % sum(matched))\n",
    "    print('Matched tweet full text\\n')\n",
    "    print(dfall[matched]['original_fulltext'].values)\n",
    "    dfall = dfall[-matched].reset_index(drop=True)\n",
    "    print('Remained dfall size: %d tweets' % dfall.shape[0])\n",
    "    print('*****************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to get the number of redundant tweet per userID\n",
    "dfall['num_RedunTweet_perUser'] = dfall.groupby(['userID', 'fulltext'])['id'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-meter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to get tweets posted by users who have more than 2 redundant tweets \n",
    "dfall['MoreThan2RedunTweets_perUser']= dfall['num_RedunTweet_perUser']>2\n",
    "dfall['MoreThan2RedunTweets_perUser'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify tweets posted by 1% and 5% users\n",
    "def topUremove(df):\n",
    "    # Number of tweets per user in current (or remained) dataframe\n",
    "    df['numTweet_perUser'] = df.groupby(['userID'])['id'].transform('count')\n",
    "\n",
    "    # Top 5% most numTweet users\n",
    "    numUsers = len(df['userID'].unique())\n",
    "    rank5 = int(np.ceil(numUsers*0.05))\n",
    "    rank5numTweetCutoff = sorted(df.groupby('userID')['numTweet_perUser'].first().to_list(), reverse=True)[rank5]\n",
    "    print('Top 5% User removed when numTweet_perUser is greater than %d' % rank5numTweetCutoff)\n",
    "    df['rank5remove'] = df['numTweet_perUser']>=rank5numTweetCutoff\n",
    "    print(df['rank5remove'].value_counts())\n",
    "    \n",
    "    # Top 1% most numTweet users\n",
    "    numUsers = len(df['userID'].unique())\n",
    "    rank1 = int(np.ceil(numUsers*0.01))\n",
    "    rank1numTweetCutoff = sorted(df.groupby('userID')['numTweet_perUser'].first().to_list(), reverse=True)[rank1]\n",
    "    print('Top 1% User removed when numTweet_perUser is greater than %d' % rank1numTweetCutoff)\n",
    "    df['rank1remove'] = df['numTweet_perUser']>=rank1numTweetCutoff\n",
    "    print(df['rank1remove'].value_counts())\n",
    "\n",
    "    return df\n",
    "\n",
    "dfe_tmp = topUremove(dfall[dfall['COVID']=='pre'])\n",
    "dfo_tmp = topUremove(dfall[dfall['COVID']=='post'])\n",
    "dfall = pd.concat([dfe_tmp, dfo_tmp], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column that show number of words in fulltext \n",
    "dfall['num_vocab'] = dfall['fulltext'].apply(lambda x: len(x.split(' ')) if x else 0)\n",
    "dfall['atleast_3_vocab']=dfall['num_vocab']>=3\n",
    "dfall['atleast_3_vocab'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tweets posted by users who generated at least 2 redundant tweet\n",
    "# remove tweets posted by top 1% users, tweets contains less than 3 words  \n",
    "# select dataset to analysis if needed, i.e., pre- pr post- COVID \n",
    "dfselo = dfall[-(dfall['MoreThan2RedunTweets_perUser']) & -(dfall['rank1remove']) & (dfall['num_vocab']>=3) & (dfall['COVID']=='pre')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-language",
   "metadata": {},
   "source": [
    "## 2nd Cleaning text content of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe for a park\n",
    "dfsel_park = dfselo.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweet cleaning - tweet-preprocessor 0.6.0 \n",
    "# Executed cleaning using default, which went through all options\n",
    "#URL\tp.OPT.URL\n",
    "#Mention\tp.OPT.MENTION\n",
    "#Hashtag\tp.OPT.HASHTAG\n",
    "#Reserved Wordsp.OPT.RESERVED\n",
    "#Emoji\tp.OPT.EMOJI\n",
    "#Smiley\tp.OPT.SMILEY\n",
    "#Number\tp.OPT.NUMBER\n",
    "\n",
    "dfsel_park['cleantext'] = dfsel_park['fulltext'].apply(lambda x: p.clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-synthetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to get the number of redundant cleantweet per userID\n",
    "# Exclude tweets posted by users who generated at least 2 redundant tweet\n",
    "# Some tweets generated by bots may not be completely the same, we can use this process to remove more potential bot tweets\n",
    "\n",
    "dfsel_park['num_RedunCleanTweet_perUser'] = dfsel_park.groupby(['cleantext'])['id'].transform('count')\n",
    "dfsel_park = dfsel_park[dfsel_park['num_RedunCleanTweet_perUser']<2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK module\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].str.replace('[0-9]+', '')\n",
    "# all text to lower case\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text) #keep English Character\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-retreat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: remove_punc(x))\n",
    "#dfsel['hashtag_seg'] = dfsel['hashtag_seg'].apply(lambda x: remove_punc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-jurisdiction",
   "metadata": {},
   "source": [
    "## Make n-gram with nltk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ambient-event",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(dfsel_park['cleantext'].str.contains('childrens zoo',flags=re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfsel_park[dfsel_park['cleantext'].str.contains('bronx zoo holiday',flags=re.IGNORECASE)]['fulltext'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents([doc.split() for doc in dfsel_park['cleantext']])\n",
    "# Filter only those that occur at least 50 times\n",
    "finder.apply_freq_filter(5)\n",
    "bigram_scores = finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_documents([doc.split() for doc in dfsel_park['cleantext']])\n",
    "# Filter only those that occur at least 50 times\n",
    "finder.apply_freq_filter(5)\n",
    "trigram_scores = finder.score_ngrams(trigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "bigram_pmi.columns = ['bigram', 'pmi']\n",
    "bigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)\n",
    "\n",
    "trigram_pmi = pd.DataFrame(trigram_scores)\n",
    "trigram_pmi.columns = ['trigram', 'pmi']\n",
    "trigram_pmi.sort_values(by='pmi', axis = 0, ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civic-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word_list = set(stopwords.words('english'))\n",
    "# Filter for bigrams with only noun-type structures\n",
    "def bigram_filter(bigram):\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in stop_word_list or bigram[1] in stop_word_list:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Filter for trigrams with only noun-type structures\n",
    "def trigram_filter(trigram):\n",
    "    tag = nltk.pos_tag(trigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n",
    "        return False\n",
    "    if trigram[0] in stop_word_list or trigram[-1] in stop_word_list or trigram[1] in stop_word_list:\n",
    "        return False\n",
    "    if 'n' in trigram or 't' in trigram:\n",
    "         return False\n",
    "    if 'PRON' in trigram:\n",
    "        return False\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n",
    "# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n",
    "filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n",
    "                                              bigram_filter(bigram['bigram'])\\\n",
    "                                              and bigram.pmi > 5, axis = 1)][:500]\n",
    "\n",
    "filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n",
    "                                                 trigram_filter(trigram['trigram'])\\\n",
    "                                                 and trigram.pmi > 5, axis = 1)][:500]\n",
    "\n",
    "\n",
    "bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n",
    "trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-airfare",
   "metadata": {},
   "source": [
    "## Curate ngram words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_del = ['open air', 'cant wait', 'th stprospect', 'join us', \n",
    "#              'bandshell whats', 'th annual', 'ive seen', 'past saturday',\n",
    "#              'july th', 'last night', 'th street', 'feel like',\n",
    "#              'dont know', 'im gonna', 'last week', 'last weekend',\n",
    "#              'look like', 'im going']\n",
    "#\n",
    "#bigrams = [e for e in bigrams if e not in bigram_del]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated bigrams after word analyses\n",
    "\n",
    "bigrams = [\n",
    " 'nybg',\n",
    " 'bronx park',\n",
    " 'bronx river',\n",
    " 'bronx zoo',\n",
    " 'van cortlandt',\n",
    " 'orchid show', # redundant\n",
    " 'cherry blossoms',\n",
    " 'brady playground',\n",
    " 'pelham bay',\n",
    " 'dinosaur safari',\n",
    " 'living art',\n",
    " 'sea lion',\n",
    " 'last night',\n",
    " 'happy birthday',\n",
    " 'holiday light',\n",
    " 'holiday train',\n",
    " 'mothers day',\n",
    " 'train show',\n",
    " #'orchid show', # redundant\n",
    " 'family fun',\n",
    " 'new york',\n",
    " 'botanical garden',\n",
    " 'rock garden',\n",
    " 'rose garden',\n",
    " 'york city',\n",
    " 'east bronx'\n",
    "]\n",
    "\n",
    "# make these singular: q trains, water fountains, bike lanes, new years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "discrete-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curated trigrams after word analyses\n",
    "trigrams = [\n",
    " 'summer end cityscape',\n",
    " 'roberto burle mar',\n",
    " 'rockefeller rose garden',\n",
    " 'holiday train show',\n",
    " 'bronx park east',\n",
    " 'New York Botanical Garden',\n",
    " 'Botanical Garden',\n",
    " 'new york city',\n",
    " 'ny botanical garden'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create n-grams. updated on 0618.2021 \n",
    "def create_ngram(x):\n",
    "    ngram = []\n",
    "    pickedgrams = []\n",
    "    for gram in trigrams:\n",
    "        if gram in x:\n",
    "            pickedgrams.append(gram)\n",
    "            ngram.append('_'.join(gram.split()))\n",
    "    for gram in bigrams:\n",
    "        anchor=True\n",
    "        for p in pickedgrams:\n",
    "            if (gram in p):\n",
    "                anchor=False\n",
    "        if anchor and (gram in x):\n",
    "            ngram.append('_'.join(gram.split()))\n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate n-grams\n",
    "def replace_ngram(x):\n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '')\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manual-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move bigram/trigram to another column\n",
    "#Then delete ngram words in tweets cleantext\n",
    "# this is to avoid ngrams being deleted in the following steps\n",
    "dfsel_park['ngrams'] = dfsel_park['cleantext'].map(lambda x: create_ngram(x))\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].map(lambda x: replace_ngram(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-gamma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer using nltk\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-English words\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "def remove_nonEnglish(token):\n",
    "    return([w for w in token if w.lower() in words or not w.isalpha()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: remove_nonEnglish(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-familiar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSmallVocab(token):\n",
    "    return([w for w in token if ((len(w)>2) & (len(w)<16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('th')\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@', 'en', 'de', 'wa', 'dont', 'cant', 's', 'nt']\n",
    "stop_words = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "dfsel_park['cleantext'] = dfsel_park['cleantext'].apply(lambda x: [t for t in x if t not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer using spacy\n",
    "dfsel_park['clean_text_trigrams'] = dfsel_park['cleantext'].copy()\n",
    "allowed_tags=['NOUN', 'VERB']\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "dfsel_park['clean_text_trigrams'] = dfsel_park['clean_text_trigrams'].apply(lambda x: [token.lemma_ for token in nlp(' '.join(x)) if token.pos_ in allowed_tags])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove smallvocab\n",
    "def removeSmallVocab(token):\n",
    "    return([w for w in token if ((len(w)>2) & (len(w)<16))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-manual",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams'] = dfsel_park['clean_text_trigrams'].apply(lambda x: removeSmallVocab(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge single words and bigram/trigrams\n",
    "# this is to have bigram/trigram be added to the data for analysis\n",
    "dfsel_park['clean_text_trigrams'] = dfsel_park['clean_text_trigrams'] + dfsel_park['ngrams']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column that show number of words in fulltext \n",
    "dfsel_park['num_vocab'] = dfsel_park['clean_text_trigrams'].apply(lambda x: len(x) if x else 0)\n",
    "# remove empty rows and words length less than 2\n",
    "dfsel_park = dfsel_park[(dfsel_park['num_vocab']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut low frequency words\n",
    "\n",
    "freq_cut = 3 # np.ceil(dfsel_park.shape[0]*0.0001)\n",
    "cut_words = []\n",
    "for v, freq in fdist.most_common():\n",
    "    if freq<=freq_cut:\n",
    "        cut_words.append(v)\n",
    "       \n",
    "dfsel_park['clean_text_trigrams_cutlowfreq'] = dfsel_park['clean_text_trigrams'].apply(lambda x: [e for e in x if e not in cut_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column that show number of words in fulltext \n",
    "dfsel_park['num_vocab'] = dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: len(x) if x else 0)\n",
    "# remove empty rows and words length less than 2\n",
    "dfsel_park = dfsel_park[(dfsel_park['num_vocab']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-chinese",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at word frequency\n",
    "fdist = FreqDist([e for l in dfsel_park['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-adams",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove highfrequency words\n",
    "remove_words = ['bronx_park','day','time','today','year','let','come','park',\n",
    "                'find','take','way','thing',\n",
    "                'new_york','new_york_city','york_city','yesterday','tomorrow','doe'] #add'new yor'bc it could result in not informative topics\n",
    "\n",
    "dfsel_park['clean_text_trigrams_cutlowfreq'] = dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: [e for e in x if e not in remove_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update column that show number of words in fulltext \n",
    "dfsel_park['num_vocab'] = dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: len(x) if x else 0)\n",
    "# remove empty rows\n",
    "dfsel_park = dfsel_park[(dfsel_park['num_vocab']>=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_park['clean_text_trigrams_cutlowfreq'].apply(lambda x: len(x)).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-scott",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Base and Cleaning \n",
    "import json\n",
    "import requests\n",
    "import emoji\n",
    "import regex\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "#Visualizations\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import pyLDAvis.gensim\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py \n",
    "import chart_studio.tools as tls\n",
    "\n",
    "#Natural Language Processing (NLP)\n",
    "import spacy\n",
    "import gensim\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from wordcloud import STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "warnings.filterwarnings(\"ignore\")#, category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = Dictionary(dfsel_park['clean_text_trigrams_cutlowfreq'])\n",
    "print(len(id2word))\n",
    "corpus = [id2word.doc2bow(d) for d in dfsel_park['clean_text_trigrams_cutlowfreq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-migration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "from corextopic import corextopic as ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-banking",
   "metadata": {},
   "source": [
    "# STTM Topic modeling - GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply the GSDMM code from https://github.com/rwalk/gsdmm\n",
    "sys.path.append('../../chapter3/gits/gsdmm/')\n",
    "sys.path.append('../../chapter3/gits/gsdmm/gsdmm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gsdmm import MovieGroupProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-federal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a fit procedure examines the suitable max number of topics in the dataset using different values of k, number of topics\n",
    "ks = list(range(100,650,50))\n",
    "kstable = []\n",
    "for k in ks:\n",
    "    mgp = MovieGroupProcess(K=k, alpha=0.1, beta=0.1, n_iters=20)\n",
    "    l_id2word = set(list(id2word.values()))\n",
    "    y = mgp.fit(dfsel_park['clean_text_trigrams_cutlowfreq'].to_list(), len(l_id2word))\n",
    "    with open('../output/pre/Bronx/bronx_kw_%s.model' % k, 'wb') as f:\n",
    "        pickle.dump(mgp, f)\n",
    "        f.close()\n",
    "    clus = (np.array(mgp.cluster_doc_count)>0).sum()\n",
    "    kstable.append({'k': k,\n",
    "                    'clusters': clus})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "great-steering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a figure that display the number of clusters identified based on different k values\n",
    "#the number of clusters found by GSDMM would become stable when K is experimentally increased\n",
    "dfks = pd.DataFrame(kstable)\n",
    "fig, ax = plt.subplots(1,1, figsize=(6,4))\n",
    "ax.plot(dfks['k'], dfks['clusters'], '.-', markersize=10)\n",
    "ax.set_xlabel('K')\n",
    "ax.set_ylabel('Number of cluster found by GDSMM')\n",
    "plt.savefig('../output/pre/Bronx/bronx_pre_STTM_modeltest_0101',bbox_inches =\"tight\",pad_inches = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-houston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input the suitable K value to run the analysis\n",
    "mgp = MovieGroupProcess(K=300,alpha=0.1, beta=0.1, n_iters=100) #updated parameters\n",
    "l_id2word = set(list(id2word.values()))\n",
    "y = mgp.fit(dfsel_park['clean_text_trigrams_cutlowfreq'].to_list(), len(l_id2word))\n",
    "with open('../output/pre/Bronx/bronx_pre_0101_k300', 'wb') as f:\n",
    "    pickle.dump(mgp, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demonstrate the results\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "nt = (doc_count>0).sum()\n",
    "print('Number of topic that has tweets:', nt)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-nt:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "# Show the top 5 words in term frequency for each cluster \n",
    "#top_words(mgp.cluster_word_distribution, top_index, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "(doc_count>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "def topic_words(cwd, ti, wc=5):\n",
    "    twords = []\n",
    "    for t in ti:\n",
    "        nwd = {k: v for k, v in sorted(cwd[t].items(), key=lambda item: item[1], reverse=True)}\n",
    "        n_items = list(islice(nwd.items(), 20))\n",
    "        #print('Topic number %s' % t)\n",
    "        for k,v in n_items:\n",
    "            twords.append({'fulltext': k,\n",
    "                           'topic_prob': v})\n",
    "            #print(k, v)\n",
    "        #print('\\n')\n",
    "    return twords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupational-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_assign = []\n",
    "\n",
    "for t in dfsel_park['clean_text_trigrams_cutlowfreq'].tolist():\n",
    "    score = mgp.choose_best_label(t)\n",
    "    clu_assign.append({'topic_number': score[0],\n",
    "                       'topic_prob': score[1]})\n",
    "\n",
    "dfclu = pd.DataFrame(clu_assign)\n",
    "dfsel_ppp = pd.concat([dfsel_park.reset_index(drop=True), dfclu], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('../output/pre/Bronx/bronx_pre_sttmmodel_topic_0629.xlsx', engine='xlsxwriter') as writer:\n",
    "    for k, g in dfsel_ppp.groupby('topic_number'):\n",
    "        gtmp = g.sort_values(by='topic_prob', ascending=False).reset_index(drop=True)\n",
    "        twords = topic_words(mgp.cluster_word_distribution, [k,], 10)\n",
    "        dftw = pd.DataFrame(twords)\n",
    "        #print(dftw.head())\n",
    "        dftw = pd.concat([dftw, gtmp[['original_fulltext', 'topic_prob']]], axis=0)\n",
    "        print(dftw.shape)\n",
    "        dftw.to_excel(writer, sheet_name='Topic%s'%k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsel_ppp.to_csv('../output/pre/Bronx/tweets_STTMresult_bronx_pre_0629.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-exclusive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print wordcloud by the group of tweets related to a certain topic \n",
    "\n",
    "tweet_ALL = \" \".join([e for l in dfsel_ppp[dfsel_ppp['topic_number']==43]['clean_text_trigrams_cutlowfreq'] for e in l])\n",
    "fig, ax = plt.subplots(1,1, figsize=(30,10))\n",
    "wc = WordCloud(width=2400, height=1200, max_words=400, background_color=\"white\").generate(tweet_ALL) #max_font_size=60\n",
    "ax.imshow(wc, interpolation='none')\n",
    "fig.savefig('test_wordcloud.png', dpi=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
